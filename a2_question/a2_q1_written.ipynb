{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AUwbFEaMiWi"
   },
   "source": [
    "# COMP3314 Assignment2-Q1: Written Question (40 Points)\n",
    "\n",
    "## Single Answer Questions (20 points, 4 points each) \n",
    "\n",
    "No need to implement any code. Please fill your answers in the following table. \n",
    "\n",
    "|   Q1   |  Q2   |   Q3   |  Q4   |   Q5  |  \n",
    "|  ----  | ----  |  ----  | ----  | ----  | \n",
    "|    B   |   B   |   C    |  C    |   A   |  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1\n",
    "\n",
    "Which one of the following statements about SVM is correct?\n",
    "\n",
    "A. Hinge loss is applicable in hard margin SVM, but not in soft margin SVM. (❎ -- only applicable in soft margin SVM)\n",
    "\n",
    "B. Given an input example, SVM can not output a probability distribution over the possible labels. (☑️)\n",
    "\n",
    "C. In kernel SVM, the main purpose of “kernel trick” is to improve the accuracy of SVM. (❎ -- to accelerate projection of data into higher dimensions)\n",
    "\n",
    "D. The maximum margin decision boundary that an SVM generates has the lowest error among all linear classifiers. (❎)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsIKbwXx6f6m"
   },
   "source": [
    "### Q2\n",
    "\n",
    "Which one of the following statements about dimensionality reduction is correct? \n",
    "\n",
    "A. The projection relationship of PCA can always be represented by a 2D matrix. (❎ -- transformation matrix W is a d*k matrix, maps a d-dimension vector to a k-dimension vector)\n",
    "\n",
    "B. PCA gives the best low-rank approximation of a matrix by minimizing the sum-of-square errors.\n",
    "\n",
    "C. If we use PCA to reduce a dataset's dimension from 1000 to 500, it is usually possible to reverse the operation without information loss. (❎ -- if we use PCA to reduce dimension from 2D to 1D, of course we can't reverse it back to 2D)\n",
    "\n",
    "D. PCA is a non-deterministic algorithm. If you apply PCA to the same training data, you may get slightly different results each time. (❎ -- PCA works by mapping data points using a vector. Multiplying matrices is deterministic.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXfwg4OC8_mQ"
   },
   "source": [
    "### Q3\n",
    "\n",
    "Suppose you perform PCA on a 100-dimension dataset, setting the explained variance ratio to 65%. How many dimensions will the resulting dataset have?\n",
    "\n",
    "A. Around 35.\n",
    "\n",
    "B. Around 65.\n",
    "\n",
    "C. From 1-100, depends on data.\n",
    "\n",
    "D. Between 35 and 65."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4\n",
    "Which one of the following statements about KNN is correct? \n",
    "\n",
    "A. K denotes the number of classes. (❎ -- K denotes number of neighbours of a data point we want to classify)\n",
    "\n",
    "B. KNN is an unsupervised clustering method. (❎ -- \"KNN is unsupervised because there is no training, but it is used for classification rather than clustering\" is the best answer I can come up with. Opinions vary wildly in literature and online discussions)\n",
    "\n",
    "C. The memory requirement of the KNN algorithm increases with the growth of the training data size. (☑️ -- we cannot discard any training sample -- they may be useful to classify some other sample, so memory grows and grows)\n",
    "\n",
    "D. Increasing K improves model accuracy however makes the inference slower. (❎ -- increasing total data points, not K, makes inference slower. K only affects under-/overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5\n",
    "Which one of the following statements about decision tree is correct? \n",
    "\n",
    "A. If a decision tree is overfitted to the training data, decreasing the max-depth usually is a good solution. (☑️)\n",
    "\n",
    "B. If a decision tree is underfitted to the training data, normalizing the input data would be a good solution.\n",
    "\n",
    "C. The Gini impurity of a node is always lower than its parent node. (❎ -- no correlation)\n",
    "\n",
    "D. Mean absolute error is commonly used to measure the impurity of a node. (❎ -- Gini Impurity is commonly used to measure the impurity of a node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6 Decision Tree for Movie Preferences (20 Points)\n",
    "\n",
    "You are working on a project to predict whether a person prefers a particular genre of movies based on their characteristics. Consider a dataset of moviegoers with the following features:\n",
    "\n",
    "- Genre: {Action, Drama}\n",
    "- Age Group: {Teen, Adult}\n",
    "- Ticket Price: {Regular, VIP}\n",
    "\n",
    "Here is the dataset:\n",
    "\n",
    "| Genre  | Age Group | Ticket Price | Preference? |\n",
    "|--------|-----------|--------------|-------------|\n",
    "| Action | Adult     | VIP          | Yes         |\n",
    "| Drama  | Adult     | Regular      | Yes         |\n",
    "| Drama  | Adult     | Regular      | Yes         |\n",
    "| Action | Adult     | VIP          | No          |\n",
    "| Action | Teen      | Regular      | No          |\n",
    "| Action | Adult     | Regular      | Yes         |\n",
    "| Drama  | Teen      | Regular      | Yes         |\n",
    "| Drama  | Teen      | VIP          | Yes         |\n",
    "| Action | Teen      | Regular      | Yes         |\n",
    "| Drama  | Adult     | Regular      | No          |\n",
    "| Drama  | Adult     | VIP          | Yes         |\n",
    "| Drama  | Teen      | Regular      | Yes         |\n",
    "| Drama  | Adult     | VIP          | No          |\n",
    "| Drama  | Teen      | VIP          | Yes         |\n",
    "| Action | Teen      | VIP          | No          |\n",
    "| Action | Teen      | VIP          | No          |\n",
    "\n",
    "Note that samples with the same features can have different labels. If the leaf node of a decision tree is not pure, the majority vote is used to determine the output label.\n",
    "\n",
    "Now, your goal is to build a decision tree to predict whether a person prefers a particular genre of movies. In particular, you want to know which feature (among Genre, Age Group, and Ticket Price) is the most important feature to predict the preference. In other words, which feature should be the root node of the decision tree to maximize the information gain?\n",
    "\n",
    "Your tasks:\n",
    "\n",
    "1. Compute Gini impurity at the root node. (5 points)\n",
    "2. For the 3 features (Genre, Age Group, and Ticket Price), compute the information gain if that feature is used to split the root node. (10 points)\n",
    "3. Conclude which feature is the most important feature to predict the preference as it maximizes the information gain. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. At the root node,\n",
    "\n",
    "$ I_G(root) = 1 - (p(Yes))^2 - (p(No))^2 = 1 - (10/16)^2 - (6/16)^2 = 0.469 $\n",
    "\n",
    "2. \n",
    "\n",
    "$ IG(root, \"Genre\\ is\\ Action\") = I_G(root) - N_{left}/N_{root} * I_G(left) - N_{right}/N_{root} * I_G(right) $\n",
    "\n",
    "$ \\ \\ \\ \\ where\\ I_G(right) = 1 - (3/7)^2 - (4/7)^2 = 0.490 $\n",
    "\n",
    "$ \\ \\ \\ \\ and\\ I_G(left) = 1 - (7/9)^2 - (2/9)^2 = 0.346 $\n",
    "\n",
    "$ \\ \\ \\ \\ so\\ IG(root, \"Genre\\ is\\ Action\") = 0.06 $\n",
    "\n",
    "$ IG(root, \"Age\\ Group\\ is\\ Adult\") = I_G(root) - N_{left}/N_{root} * I_G(left) - N_{right}/N_{root} * I_G(right) $\n",
    "\n",
    "$ \\ \\ \\ \\ where\\ I_G(right) = 1 - (5/8)^2 - (3/8)^2 = 0.469 $\n",
    "\n",
    "$ \\ \\ \\ \\ and\\ I_G(left) = 1 - (5/8)^2 - (3/8)^2 = 0.469 $\n",
    "\n",
    "$ \\ \\ \\ \\ so\\ IG(root, \"Age\\ Group\\ is\\ Adult\") = 0 $\n",
    "\n",
    "$ IG(root, \"Ticket\\ Price\\ is\\ VIP\") = I_G(root) - N_{left}/N_{root} * I_G(left) - N_{right}/N_{root} * I_G(right) $\n",
    "\n",
    "$ \\ \\ \\ \\ where\\ I_G(right) = 1 - (4/8)^2 - (4/8)^2 = 0.5 $\n",
    "\n",
    "$ \\ \\ \\ \\ and\\ I_G(left) = 1 - (6/8)^2 - (2/8)^2 = 0.375 $\n",
    "\n",
    "$ \\ \\ \\ \\ so\\ IG(root, \"Ticket\\ Price\\ is\\ VIP\") = 0.0315 $\n",
    "\n",
    "3. Splitting by genre produces greatest information gain, so genre is the most important feature to predict preference"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
